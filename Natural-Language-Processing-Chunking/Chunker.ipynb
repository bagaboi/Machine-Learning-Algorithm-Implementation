{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word chunking based on POS tags\n",
    "The motivation to perform chunking is to get a set of words with nouns which can help in finding nouns with adjectives and if found, those adjectives can be checked for any aggregation on the noun attributes. Eg: the average age, the minimum salary, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk.util import tree2conlltags,conlltags2tree\n",
    "from nltk.tag import UnigramTagger,BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoNLL2000 data\n",
    "The CoNLL2000 data contains chunked sentences which will be used to train the chunker model\n",
    "\n",
    "The data has sentences whose words are divided into chunks(collection of words along with individual POS tags of words and the chunk POS tags. Below are a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700 248\n",
      "(S\n",
      "  (NP The/DT August/NNP deficit/NN)\n",
      "  and/CC\n",
      "  (NP the/DT #/# 2.2/CD billion/CD gap/NN)\n",
      "  (VP registered/VBN)\n",
      "  (PP in/IN)\n",
      "  (NP July/NNP)\n",
      "  (VP are/VBP topped/VBN)\n",
      "  only/RB\n",
      "  (PP by/IN)\n",
      "  (NP the/DT #/# 2.3/CD billion/CD deficit/NN)\n",
      "  (PP of/IN)\n",
      "  (NP October/NNP 1988/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT first/JJ two/CD games/NNS)\n",
      "  (PP of/IN)\n",
      "  (NP the/DT World/NNP Series/NNP)\n",
      "  (PP between/IN)\n",
      "  (NP\n",
      "    the/DT\n",
      "    Oakland/NNP\n",
      "    Athletics/NNP\n",
      "    and/CC\n",
      "    San/NNP\n",
      "    Francisco/NNP\n",
      "    Giants/NNP)\n",
      "  (VP did/VBD n't/RB finish/VB)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT top/JJ 10/CD)\n",
      "  ;/:\n",
      "  instead/RB\n",
      "  (NP they/PRP)\n",
      "  (VP landed/VBD)\n",
      "  (PP in/IN)\n",
      "  (NP 16th/JJ and/CC 18th/JJ place/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "data=conll2000.chunked_sents()\n",
    "train=data[:10700]\n",
    "test=data[10700:]\n",
    "print(len(train),len(test))\n",
    "print(train[7])\n",
    "print(test[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image1.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conll_tag_chunks function uses the function tree2conlltags() which converts the chunked sentence to word, tag and chunk-tag.\n",
    "There are 3 types of chunk tags:<br>\n",
    "1 B-  which is the beginning of a chunk<br>\n",
    "2 I- which is in between of a chunk <br>\n",
    "3 O which is without a chunk<br>\n",
    "The conll_tag_chunks function then returns the individual and chunk tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT', 'B-NP'),\n",
       " ('first', 'JJ', 'I-NP'),\n",
       " ('two', 'CD', 'I-NP'),\n",
       " ('games', 'NNS', 'I-NP'),\n",
       " ('of', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('World', 'NNP', 'I-NP'),\n",
       " ('Series', 'NNP', 'I-NP'),\n",
       " ('between', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('Oakland', 'NNP', 'I-NP'),\n",
       " ('Athletics', 'NNP', 'I-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('San', 'NNP', 'I-NP'),\n",
       " ('Francisco', 'NNP', 'I-NP'),\n",
       " ('Giants', 'NNP', 'I-NP'),\n",
       " ('did', 'VBD', 'B-VP'),\n",
       " (\"n't\", 'RB', 'I-VP'),\n",
       " ('finish', 'VB', 'I-VP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('top', 'JJ', 'I-NP'),\n",
       " ('10', 'CD', 'I-NP'),\n",
       " (';', ':', 'O'),\n",
       " ('instead', 'RB', 'O'),\n",
       " ('they', 'PRP', 'B-NP'),\n",
       " ('landed', 'VBD', 'B-VP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('16th', 'JJ', 'B-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('18th', 'JJ', 'I-NP'),\n",
       " ('place', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc=tree2conlltags(test[3])\n",
    "\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined_tagger function performs a sequential tagging using the best taggers among Unigram and Bigram taggers to tag the words based on context. <br>\n",
    "Unigram tagger is an NLTK library's Part-of-Speech tagger which uses a corpus of word and POS tag to train the data based on only the current word probability of having a certain tag. Similarly Bigram tagger uses a corpus of word and POS tag to train the data based on the current and the previous word. <br>\n",
    "It uses Hidden Markov Model to train the model according to which, the maximum of the probabilities of the word having different POS tags is considered which helps us decide the approriate POS tag for the word. These probabilities depend solely on that word and the n-previous words before it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramTagChunker(ChunkParserI):\n",
    "    def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\n",
    "        train_sent_tags=conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)\n",
    "    def parse(self,tagged_sentence):\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags=[tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags=self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]\n",
    "        return conlltags2tree(wpc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse() function uses the combined_tagger model to get the appropriate chunks and then return the chunks in the original tree form using conlltags2tree() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  90.0%%\n",
      "    Precision:     81.4%%\n",
      "    Recall:        86.1%%\n",
      "    F-Measure:     83.7%%\n"
     ]
    }
   ],
   "source": [
    "ntc=NGramTagChunker(train)\n",
    "print(ntc.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test some of the test sentences on our chunker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  -LRB-/(\n",
      "  (NP A/DT ratings/NNS point/VBP)\n",
      "  (VP represents/VBZ)\n",
      "  (NP 904,000/CD television/NN households/NNS)\n",
      "  ;/:\n",
      "  (NP shares/NNS)\n",
      "  (VP indicate/VBP)\n",
      "  (NP the/DT percentage/NN)\n",
      "  (PP of/IN)\n",
      "  (NP sets/NNS)\n",
      "  (PP in/IN)\n",
      "  (NP use/NN)\n",
      "  ./.\n",
      "  -RRB-/))\n"
     ]
    }
   ],
   "source": [
    "print(test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image2.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT ratings/NNS point/NN)\n",
      "  (VP represents/VBZ)\n",
      "  (NP 904,000/CD television/NN households/NNS)\n",
      "  ;/:\n",
      "  (NP shares/NNS)\n",
      "  (VP indicate/VBP)\n",
      "  (NP the/DT percentage/NN)\n",
      "  (PP of/IN)\n",
      "  (NP sets/NNS)\n",
      "  (PP in/IN)\n",
      "  (NP use/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence='A ratings point represents 904,000 television households ; shares indicate the percentage of sets in use .'\n",
    "\n",
    "nltk_pos_tagged=nltk.pos_tag(sentence.split())\n",
    "chunk_tree=ntc.parse(nltk_pos_tagged)\n",
    "print(chunk_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP CBS/NNP)\n",
      "  (VP held/VBD)\n",
      "  (NP the/DT previous/JJ record/NN)\n",
      "  (PP for/IN)\n",
      "  (NP consecutive/JJ No./NN 1/CD victories/NNS)\n",
      "  --/:\n",
      "  (NP 46/CD weeks/NNS)\n",
      "  --/:\n",
      "  (PP during/IN)\n",
      "  (NP the/DT 1962-63/CD season/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(test[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image3.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP CBS/NNP)\n",
      "  (VP held/VBD)\n",
      "  (NP the/DT previous/JJ record/NN)\n",
      "  (PP for/IN)\n",
      "  (NP consecutive/JJ No./NN 1/CD victories/NNS)\n",
      "  --/:\n",
      "  (NP 46/CD weeks/NNS)\n",
      "  --/:\n",
      "  (PP during/IN)\n",
      "  (NP the/DT 1962-63/JJ season/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence='CBS held the previous record for consecutive No. 1 victories -- 46 weeks -- during the 1962-63 season .'\n",
    "\n",
    "nltk_pos_tagged=nltk.pos_tag(sentence.split())\n",
    "chunk_tree=ntc.parse(nltk_pos_tagged)\n",
    "print(chunk_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP what/WP)\n",
      "  (VP is/VBZ)\n",
      "  (NP the/DT minimum/JJ salary/NN)\n",
      "  (PP of/IN)\n",
      "  (NP employee/NN)\n",
      "  (NP whose/WP$ age/NN)\n",
      "  (VP is/VBZ)\n",
      "  (NP greater/JJR)\n",
      "  (PP than/IN)\n",
      "  (NP 29/CD))\n"
     ]
    }
   ],
   "source": [
    "sentence='what is the minimum salary of employee whose age is greater than 29'\n",
    "nltk_pos_tagged=nltk.pos_tag(sentence.split())\n",
    "chunk_tree=ntc.parse(nltk_pos_tagged)\n",
    "print(chunk_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
